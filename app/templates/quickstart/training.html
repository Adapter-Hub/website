<h3 class="mt-0">Train an Adapter 🏋️️</h3>
<p>Training a new task adapter requires only few modifications compared to fully fine-tuning a model with Hugging Face's <code>Trainer</code>.
    We first load a pre-trained model, e.g., <code>roberta-base</code> and add a new task adapter:</p>
<pre class="code">model = AutoModelWithHeads.from_pretrained('roberta-base')
model.add_adapter("sst-2", AdapterType.text_task)
model.train_adapter(["sst-2"])
</pre>
<p>By calling <code>train_adapter(["sst-2"])</code> we freeze all transformer parameters except for the parameters of sst-2 adapter.
Before training we add a new classification head to our model:</p>
<pre class="code">model.add_classification_head("sst-2", num_labels=2)
model.set_active_adapters([["sst-2"]])
</pre>
<p>The weights of this classification head can be stored together with the adapter weights to allow for a full reproducibility.
The method call <code>model.set_active_adapters([["sst-2"]])</code> registers the sst-2 adapter as a default for training. This also supports adapter stacking and adapter fusion!</p>
<p>We can then train our adapter using the Hugging Face <code>Trainer</code>:</p>

<p class="alert alert-info"> 💡Tip1💡️ </br> Adapter weights are usually initialized randomly. That is why we require a higher learning rate.
We have found that a default adapter learning rate of <code>lr=0.0001</code> works well for most settings. </p>
<p class="alert alert-info"> 💡Tip2💡️ </br> Depending on your data set size you might also need to train longer than usual.
 To avoid overfitting you can evaluating the adapters after each epoch on the development set and only save the best model. </p>
<pre class="code">trainer.train()
model.save_all_adapters('output-path')</pre>
<p>That's it! <code>model.save_all_adapters('output-path)</code> exports all adapters. Consider sharing them on AdapterHub!</p>