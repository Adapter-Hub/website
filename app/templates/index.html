{% extends 'base.html' %}

{% block title %} Home {% endblock %}

{% block header %}
    <div class="row">
        <div class="text-light col-md-9">
            <p class="highlight-text">
                <span class="highlight">AdapterHub</span>
                is a <span class="highlight">central repository</span><br class="d-none d-lg-inline">
                for pre-trained <span class="highlight">adapter modules</span> for language models.
            </p>

            <div id="IndexButtonRow" class="rounded">
                <a class="btn"
                   href="{{ url_for('main.explore_tasks') }}">
                    <div>
                        <i class="fas fa-binoculars"></i>
                    </div>
                    Explore
                </a>
                <a class="btn d-none d-sm-inline-block"
                   href="{{ config.CONTRIBUTING_URL }}">
                    <div>
                        <i class="fas fa-upload"></i>
                    </div>
                    Upload
                </a>
                <a class="btn"
                   href="{{ config.DOCUMENTATION_URL }}">
                    <div>
                        <i class="fas fa-book"></i>
                    </div>
                    Docs
                </a>
                <a class="btn d-none d-lg-inline-block"
                   href="https://github.com/adapter-hub">
                    <div>
                        <i class="fab fa-github"></i>
                    </div>
                    GitHub
                </a>
                <a class="btn"
                   href="https://github.com/adapter-hub">
                    <div>
                        <i class="fas fa-scroll"></i>
                    </div>
                    Paper
                </a>
            </div>
        </div>
        <div class="col-sm-3 text-right d-none d-md-block">
            <img src="{{ url_for('static', filename='adapter-bert.png') }}" height="225"/>
        </div>
    </div>
{% endblock %}

{% block content %}

    <section class="">
        <div class="row">
            <div class="card col-md mt-3 mx-3 bg-light border-0">
                <div class="card-body">
                    <h5 class="card-title">Adapters ü§ñ</h5>
                    <p class="card-text">Adapters are a small amount of newly introduced weights within each layer of a transformer model such as BERT or RoBERTa. Training Adapters performs on-par with full fine-tuning the model, while only requiring as little as 1MB of storage space per task. </p>
                </div>
            </div>

            <div class="card col-md mx-3 ml-md-0 mr-md-3 mr-lg-0 mt-3 bg-light border-0">
                <div class="card-body">
                    <h5 class="card-title">Modular, Composable, Extensible üîß</h5>
                    <p class="card-text"> Adapters are encapsuled within layers, making the  representations  compatible. They can thus be stacked, composed or similarly modified, opening up many research directions. </p>
                </div>
            </div>

            <div class="card col-lg mx-3 mt-3 bg-light border-0">
                <div class="card-body">
                    <h5 class="card-title">Built on HuggingFace Transformers üöÄ</h5>
                    <p class="card-text">AdapterHub builds on the <a href=https://github.com/huggingface/transformers target="_blank">HuggingFace transformers</a> framework requiring as little as two additional lines of code to train adapters for a downstream task.</p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="row">
            <div class="col col-lg-8">
                <h1>Quickstart üî•</h1>
                <p>In the following, we outline the two most common ways of using AdapterHub.
                    Install the adapter-transformers library first:
                </p>
                <p>
                    <pre class="bg-light py-3 px-4">pip install git+https://github.com/adapter-hub/adapter-transformers.git</pre>
                </p>
            </div>
        </div>

        <div class="row mt-3">
            <div class="col-sm-6">
                <div class="card">
                    <div class="card-body">
                        <h4 class="card-title">Load an Adapter üèÑ</h4>
                        <p>Loading existing adapters from our repository is as simple as adding one additional line of code!</p>
                        <pre class="bg-light p-3">model = AutoModel.from_pretrained('roberta-base')
model.load_adapter('sst', load_head=True)
</pre>
                        <p>This loads an adapter that is trained on the Stanford Sentiment Treebank.
                            It achieves state-of-the-art results while also being light-weight: the <a href="">SST adapter</a> is only 3MB!</p>
                        <p>
                            We can now use this adapter to predict the sentiment of sentences:</p>
                        <pre class="bg-light p-3">
tokenizer = AutoTokenizer.from_pretrained('roberta-base')
tokens = tokenizer.tokenize("AdapterHub is awesome!")
input_tensor = torch.tensor([
    tokenizer.convert_tokens_to_ids(tokens)
])

outputs = model(
    input_tensor,
    adapter_tasks=['sst'],
    task='sst'
)</pre>
                    </div>
                </div>
            </div>
            <div class="col-sm-6">
                <div class="card">
                    <div class="card-body">
                        <h5 class="card-title">Train an Adapter üèãÔ∏èÔ∏è</h5>
                        <p class="card-text">...</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section>
        <h1>Citation üìù</h1>
        <pre class="p-4 bg-light">
@article{pfeiffer2020AdapterHub,
    title={AdapterHub},
    author={Jonas Pfeiffer, 
            Andreas R\"uckl\'{e}, 
            Clifton Poth, 
            Aishwarya Kamath,  
            Ivan Vuli\'{c}, 
            Sebastian Ruder, 
            Kyunghyun Cho, 
            Iryna Gurevych},
    journal={ArXiv},
    year={2020}
}</pre>
    </section>

{% endblock %}
